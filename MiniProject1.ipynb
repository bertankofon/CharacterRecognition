{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMNUuhRjIraT/s9ufDgkQAu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bertankofon/CharacterRecognition/blob/main/MiniProject1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Preparation"
      ],
      "metadata": {
        "id": "Cx4B5OlyrbMw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fb-_aI97W8Nf",
        "outputId": "1e380bad-b70b-49e9-cfcc-97f984727628"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 1s 0us/step\n",
            "Training images shape: (60000, 784)\n",
            "Test images shape: (10000, 784)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from keras.datasets import mnist\n",
        "\n",
        "# Load the dataset\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "\n",
        "# Normalize the images.\n",
        "train_images = (train_images / 255) - 0.5\n",
        "test_images = (test_images / 255) - 0.5\n",
        "\n",
        "# Flatten the images.\n",
        "train_images = train_images.reshape((-1, 784))\n",
        "test_images = test_images.reshape((-1, 784))\n",
        "\n",
        "print(\"Training images shape:\", train_images.shape) # Should be (60000, 784)\n",
        "print(\"Test images shape:\", test_images.shape) # Should be (10000, 784)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model Implemantation and Training"
      ],
      "metadata": {
        "id": "xXh1Mk9lr-B1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Case 1: Use tanh for all activations.\n",
        "\n",
        "Case 2: Use ReLU for the hidden layer activation and sigmoid for the output layer activation."
      ],
      "metadata": {
        "id": "WWXaHXRfU8UG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define Activation Functions and their derivatives"
      ],
      "metadata": {
        "id": "5TChwEvgjYc4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tanh activation and its derivative\n",
        "def tanh(x):\n",
        "    return np.tanh(x)\n",
        "\n",
        "def tanh_derivative(x):\n",
        "    return 1 - np.tanh(x)**2\n",
        "\n",
        "# ReLU activation and its derivative\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def relu_derivative(x):\n",
        "    return (x > 0).astype(float)\n",
        "\n",
        "# Define the sigmoid activation function\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "# Derivative of the sigmoid function for backpropagation\n",
        "def sigmoid_derivative(x):\n",
        "    return sigmoid(x) * (1 - sigmoid(x))\n",
        "\n"
      ],
      "metadata": {
        "id": "ubUDfynAVP2o"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define variable parameters"
      ],
      "metadata": {
        "id": "awB8lyOdYMHd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the learning rate\n",
        "learning_rate = 1e-2 # CHANGABLE: 1e-2, 5e-2, 9e-2: 0.01, 0.05, 0.09\n",
        "\n",
        "hidden_size = 300  # N = (300, 500 or 1000) CHANGABLE\n"
      ],
      "metadata": {
        "id": "469cf2SAYLi2"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Case 1: Use tanh for all activations."
      ],
      "metadata": {
        "id": "nnHbKncVU92C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Initialize parameters to random values\n",
        "input_size = 784  # MNIST images are 28x28 pixels\n",
        "hidden_size = 1000  # N = (300, 500 or 1000) CHANGABLE\n",
        "num_classes = 10  # There are 10 classes for the digits 0-9\n",
        "\n",
        "# Weights and biases for the hidden layer\n",
        "W1 = np.random.randn(input_size, hidden_size) * 0.01\n",
        "b1 = np.zeros((1, hidden_size))\n",
        "\n",
        "# Weights and biases for the output layer\n",
        "W2 = np.random.randn(hidden_size, num_classes) * 0.01\n",
        "b2 = np.zeros((1, num_classes))\n",
        "\n",
        "# Define the learning rate\n",
        "learning_rate = 1e-2 # CHANGABLE: 1e-2, 5e-2, 9e-2: 0.01, 0.05, 0.09\n",
        "\n",
        "# Forward pass using tanh\n",
        "def forward_pass_tanh(X, W1, b1, W2, b2):\n",
        "    Z1 = np.dot(X, W1) + b1\n",
        "    A1 = tanh(Z1)\n",
        "    Z2 = np.dot(A1, W2) + b2\n",
        "    A2 = tanh(Z2)\n",
        "    return Z1, A1, Z2, A2\n",
        "\n",
        "# Convert labels to the desired target outputs for case 1 with tanh\n",
        "def convert_labels_for_tanh(Y, num_classes=10):\n",
        "    # Initialize the matrix with -1 (since tanh outputs are in the range [-1, 1])\n",
        "    targets = -np.ones((Y.size, num_classes))\n",
        "    # Set the appropriate element for each label to 1\n",
        "    for index, label in enumerate(Y):\n",
        "        targets[index, label] = 1\n",
        "    return targets\n",
        "\n",
        "# Modified labels for tanh\n",
        "train_labels_tanh = convert_labels_for_tanh(train_labels)\n",
        "\n",
        "# Mean Squared Error Loss function\n",
        "def mse_loss(Y, A2):\n",
        "    m = Y.shape[0]\n",
        "    loss = np.sum((Y - A2)**2) / m\n",
        "    return loss\n",
        "\n",
        "\n",
        "\n",
        "# Backward pass\n",
        "def backward_pass_tanh(X, Y, Z1, A1, W2, A2):\n",
        "    m = X.shape[0]\n",
        "\n",
        "    # Gradient of MSE loss with respect to A2\n",
        "    dA2 = 2 * (A2 - Y) / m\n",
        "\n",
        "    # Gradient of the tanh activation function\n",
        "    dZ2 = dA2 * (1 - np.square(np.tanh(Z2)))\n",
        "\n",
        "    # Calculate gradients for W2 and b2\n",
        "    dW2 = np.dot(A1.T, dZ2)\n",
        "    db2 = np.sum(dZ2, axis=0, keepdims=True)\n",
        "\n",
        "    # Calculate gradients for W1 and b1\n",
        "    dA1 = np.dot(dZ2, W2.T)\n",
        "    dZ1 = dA1 * (1 - np.square(np.tanh(Z1)))\n",
        "    dW1 = np.dot(X.T, dZ1)\n",
        "    db1 = np.sum(dZ1, axis=0, keepdims=True)\n",
        "\n",
        "    return dW1, db1, dW2, db2\n",
        "\n",
        "# Parameters update function\n",
        "def update_parameters(W1, b1, W2, b2, dW1, db1, dW2, db2, learning_rate):\n",
        "    W1 -= learning_rate * dW1\n",
        "    b1 -= learning_rate * db1\n",
        "    W2 -= learning_rate * dW2\n",
        "    b2 -= learning_rate * db2\n",
        "    return W1, b1, W2, b2\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 10  # CHANGE\n",
        "for epoch in range(num_epochs):\n",
        "    # Forward pass\n",
        "    Z1, A1, Z2, A2 = forward_pass_tanh(train_images, W1, b1, W2, b2)\n",
        "\n",
        "    # Compute loss\n",
        "    cost = mse_loss(train_labels_tanh, A2)\n",
        "    print(f'Epoch {epoch + 1}, cost: {cost}')\n",
        "\n",
        "     # Backward pass\n",
        "    dW1, db1, dW2, db2 = backward_pass_tanh(train_images, train_labels_tanh, Z1, A1, W2, A2)\n",
        "\n",
        "    # Update parameters\n",
        "    W1, b1, W2, b2 = update_parameters(W1, b1, W2, b2, dW1, db1, dW2, db2, learning_rate)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hNHSCsMoVAdj",
        "outputId": "acde870f-6d83-4bf6-b00d-d0146934254c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, cost: 10.092809419516978\n",
            "Epoch 2, cost: 5.31523749401591\n",
            "Epoch 3, cost: 4.085906331680927\n",
            "Epoch 4, cost: 3.759274108608297\n",
            "Epoch 5, cost: 3.650922735608826\n",
            "Epoch 6, cost: 3.603763346314774\n",
            "Epoch 7, cost: 3.5786978942788243\n",
            "Epoch 8, cost: 3.563070113162734\n",
            "Epoch 9, cost: 3.55192747446481\n",
            "Epoch 10, cost: 3.5430622696984826\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Case 2: Use ReLU for the hidden layer activation and sigmoid for the output layer activation."
      ],
      "metadata": {
        "id": "imJuI_CxiYQo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Initialize parameters to random values\n",
        "input_size = 784  # MNIST images are 28x28 pixels\n",
        "hidden_size = 500  # N = (300, 500 or 1000) CHANGABLE\n",
        "num_classes = 10  # There are 10 classes for the digits 0-9\n",
        "\n",
        "# Weights and biases for the hidden layer\n",
        "W1 = np.random.randn(input_size, hidden_size) * 0.01\n",
        "b1 = np.zeros((1, hidden_size))\n",
        "\n",
        "# Weights and biases for the output layer\n",
        "W2 = np.random.randn(hidden_size, num_classes) * 0.01\n",
        "b2 = np.zeros((1, num_classes))\n",
        "\n",
        "# Define the learning rate\n",
        "learning_rate = 1e-1 # CHANGE\n",
        "\n",
        "# Forward pass using ReLU and sigmoid\n",
        "def forward_pass_relu_sigmoid(X, W1, b1, W2, b2):\n",
        "    Z1 = np.dot(X, W1) + b1\n",
        "    A1 = relu(Z1)\n",
        "    Z2 = np.dot(A1, W2) + b2\n",
        "    A2 = sigmoid(Z2)\n",
        "    return Z1, A1, Z2, A2\n",
        "\n",
        "# Convert labels to the desired target outputs for case 2 with sigmoid\n",
        "def convert_labels_for_sigmoid(Y, num_classes=10):\n",
        "    # Initialize the matrix with 0 (since sigmoid outputs are in the range [0, 1])\n",
        "    targets = np.zeros((Y.size, num_classes))\n",
        "    # Set the appropriate element for each label to 1\n",
        "    for index, label in enumerate(Y):\n",
        "        targets[index, label] = 1\n",
        "    return targets\n",
        "\n",
        "\n",
        "# Modified labels for tanh\n",
        "train_labels_sigmoid = convert_labels_for_sigmoid(train_labels)\n",
        "\n",
        "\n",
        "# Mean Squared Error Loss function\n",
        "def mse_loss(Y, A2):\n",
        "    m = Y.shape[0]\n",
        "    loss = np.sum((Y - A2)**2) / m\n",
        "    return loss\n",
        "\n",
        "\n",
        "# Backward pass using ReLU and sigmoid\n",
        "def backward_pass_relu_sigmoid(X, Y, Z1, A1, W2, A2):\n",
        "    m = X.shape[0]\n",
        "\n",
        "    # Gradient for W2 and b2\n",
        "    dZ2 = (A2 - Y) * sigmoid_derivative(Z2)\n",
        "    dW2 = np.dot(A1.T, dZ2) / m\n",
        "    db2 = np.sum(dZ2, axis=0, keepdims=True) / m\n",
        "\n",
        "    # Gradient for W1 and b1\n",
        "    dA1 = np.dot(dZ2, W2.T)\n",
        "    dZ1 = dA1 * relu_derivative(Z1)\n",
        "    dW1 = np.dot(X.T, dZ1) / m\n",
        "    db1 = np.sum(dZ1, axis=0, keepdims=True) / m\n",
        "\n",
        "    return dW1, db1, dW2, db2\n",
        "\n",
        "# Parameters update function\n",
        "def update_parameters(W1, b1, W2, b2, dW1, db1, dW2, db2, learning_rate):\n",
        "    W1 -= learning_rate * dW1\n",
        "    b1 -= learning_rate * db1\n",
        "    W2 -= learning_rate * dW2\n",
        "    b2 -= learning_rate * db2\n",
        "    return W1, b1, W2, b2\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 10  # CHANGE\n",
        "for epoch in range(num_epochs):\n",
        "    # Forward pass\n",
        "    Z1, A1, Z2, A2 = forward_pass_relu_sigmoid(train_images, W1, b1, W2, b2)\n",
        "\n",
        "    # Compute loss\n",
        "    cost = mse_loss(train_labels_sigmoid, A2)\n",
        "    print(f'Epoch {epoch + 1}, cost: {cost}')\n",
        "\n",
        "     # Backward pass\n",
        "    dW1, db1, dW2, db2 = backward_pass_relu_sigmoid(train_images, train_labels_sigmoid, Z1, A1, W2, A2)\n",
        "\n",
        "    # Update parameters\n",
        "    W1, b1, W2, b2 = update_parameters(W1, b1, W2, b2, dW1, db1, dW2, db2, learning_rate)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l9hm00NciZxj",
        "outputId": "aa504ab8-1d5f-448e-accc-25fbcd0c2f5f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, cost: 2.484944395132934\n",
            "Epoch 2, cost: 2.3418036565392546\n",
            "Epoch 3, cost: 2.1770137719519593\n",
            "Epoch 4, cost: 1.9563515856664788\n",
            "Epoch 5, cost: 1.6787903895018392\n",
            "Epoch 6, cost: 1.3940655779708835\n",
            "Epoch 7, cost: 1.1761348090037211\n",
            "Epoch 8, cost: 1.0482565469151446\n",
            "Epoch 9, cost: 0.9821058962071338\n",
            "Epoch 10, cost: 0.9478471339374868\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#NAIVE CODE\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Set the random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Define the sigmoid activation function\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "# Derivative of the sigmoid function for backpropagation\n",
        "def sigmoid_derivative(x):\n",
        "    return sigmoid(x) * (1 - sigmoid(x))\n",
        "\n",
        "# Initialize parameters to random values\n",
        "input_size = 784  # MNIST images are 28x28 pixels\n",
        "hidden_size = 500  # N = (300, 500 or 1000) CHANGABLE\n",
        "num_classes = 10  # There are 10 classes for the digits 0-9\n",
        "\n",
        "# Weights and biases for the hidden layer\n",
        "W1 = np.random.randn(input_size, hidden_size) * 0.01\n",
        "b1 = np.zeros((1, hidden_size))\n",
        "\n",
        "# Weights and biases for the output layer\n",
        "W2 = np.random.randn(hidden_size, num_classes) * 0.01\n",
        "b2 = np.zeros((1, num_classes))\n",
        "\n",
        "# Define the learning rate\n",
        "learning_rate = 1e-2 # CHANGABLE: 1e-2, 5e-2, 9e-2: 0.01, 0.05, 0.09\n",
        "\n",
        "# Forward pass\n",
        "def forward_pass(X, W1, b1, W2, b2):\n",
        "    Z1 = np.dot(X, W1) + b1\n",
        "    A1 = sigmoid(Z1)\n",
        "    Z2 = np.dot(A1, W2) + b2\n",
        "    A2 = np.exp(Z2) / np.sum(np.exp(Z2), axis=1, keepdims=True)  # Softmax\n",
        "    return Z1, A1, Z2, A2\n",
        "\n",
        "# Compute the cross-entropy loss\n",
        "def compute_loss(Y, A2):\n",
        "    m = Y.shape[0]\n",
        "    log_likelihood = -np.log(A2[range(m), Y])\n",
        "    loss = np.sum(log_likelihood) / m\n",
        "    return loss\n",
        "\n",
        "# Backward pass\n",
        "def backward_pass(X, Y, Z1, A1, W2, A2):\n",
        "    m = X.shape[0]\n",
        "    # Calculate W2 and b2 gradients\n",
        "    dZ2 = A2\n",
        "    dZ2[range(m), Y] -= 1\n",
        "    dW2 = np.dot(A1.T, dZ2) / m\n",
        "    db2 = np.sum(dZ2, axis=0, keepdims=True) / m\n",
        "\n",
        "    # Calculate W1 and b1 gradients\n",
        "    dA1 = np.dot(dZ2, W2.T)\n",
        "    dZ1 = dA1 * sigmoid_derivative(Z1)\n",
        "    dW1 = np.dot(X.T, dZ1) / m\n",
        "    db1 = np.sum(dZ1, axis=0, keepdims=True) / m\n",
        "\n",
        "    return dW1, db1, dW2, db2\n",
        "\n",
        "# Parameters update function\n",
        "def update_parameters(W1, b1, W2, b2, dW1, db1, dW2, db2, learning_rate):\n",
        "    W1 -= learning_rate * dW1\n",
        "    b1 -= learning_rate * db1\n",
        "    W2 -= learning_rate * dW2\n",
        "    b2 -= learning_rate * db2\n",
        "    return W1, b1, W2, b2\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 10  # You can change this\n",
        "for epoch in range(num_epochs):\n",
        "    # Forward pass\n",
        "    Z1, A1, Z2, A2 = forward_pass(train_images, W1, b1, W2, b2)\n",
        "\n",
        "    # Compute loss\n",
        "    cost = compute_loss(train_labels, A2)\n",
        "    print(f'Epoch {epoch + 1}, cost: {cost}')\n",
        "\n",
        "     # Backward pass\n",
        "    dW1, db1, dW2, db2 = backward_pass(train_images, train_labels, Z1, A1, W2, A2)\n",
        "\n",
        "    # Update parameters\n",
        "    W1, b1, W2, b2 = update_parameters(W1, b1, W2, b2, dW1, db1, dW2, db2, learning_rate)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5bdotYICsCoB",
        "outputId": "ba006ccd-53d5-4cec-f8e9-3b953028986f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, cost: 2.3054058710849628\n",
            "Epoch 2, cost: 2.3001994398006462\n",
            "Epoch 3, cost: 2.299205010144376\n",
            "Epoch 4, cost: 2.298488539925594\n",
            "Epoch 5, cost: 2.2977962863320682\n",
            "Epoch 6, cost: 2.297105549734341\n",
            "Epoch 7, cost: 2.2964135135430745\n",
            "Epoch 8, cost: 2.2957194855260736\n",
            "Epoch 9, cost: 2.2950230616566025\n",
            "Epoch 10, cost: 2.2943238795094723\n"
          ]
        }
      ]
    }
  ]
}